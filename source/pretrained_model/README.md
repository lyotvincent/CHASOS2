Description of the pre-trained model
===

# Model Tasks
1. 任务1：参考CharID第一步，深度学习基于序列预测OCR序列是否是anchor。
2. 任务2：参考CharID第二步，深度学习基于序列预测是否OCR对可以构成染色质环。
3. 任务3：预测染色质开放性。

# Input
## Data source
1. ChIA-PET
2. DNase-seq
3. ChIP-seq
## Input format
## 任务配置
对于任务1，ChIA-PET和ChIP-seq重叠的部分作为正例anchor，ChIP-seq中未重叠的CTCF富集点（确定有CTCF）乱序后作为一半反例，还有随机区域抽取作为一半反例。  
对于任务2，本模型设定的输入是一条DNA序列，所以应该无法预训练是否能成环。  
对于任务3，DNase-seq 负责确定开放区，标注0，1。  
## 抽取样本方案
理想情况下，任务1的，正例:乱序正例的反例:随机抽取的反例=2:1:1。  
但考虑开放区特征后，乱序正例的反例无法确定开放性，所以只能抽取在ChIA-PET和ChIP-seq标注区域外的序列作为反例，所以正例:非anchor区的反例=1:1。  
同时此时的理想状态是，开放正例：不开放正例：开放反例：不开放反例=1:1:1:1。这种情况疑似不太可能出现，因为anchor很可能和开放正相关。  
数据存于ROOT_DIR/data/pretrained_data  

# 预训练模型一般步骤
预训练模型的微调（fine-tuning）通常是通过在特定任务上对预训练模型进行有监督训练来完成的。以下是预训练模型微调的一般步骤：

准备数据集：收集一个适当规模的、针对特定任务的数据集，将其分为训练、验证和测试集。

加载预训练模型：使用已经训练好的预训练模型（如BERT、GPT、RoBERTa等）作为起点，通过微调来适应特定任务的需求。

替换顶层结构：将预训练模型的顶层结构（如分类器）替换为与目标任务相适应的新结构。通常，这个新结构由几个全连接层组成，以便处理目标任务的输出。

冻结预训练模型参数：在微调开始之前，将预训练模型中除了新添加的顶层结构以外的所有参数冻结，以避免在微调期间对其进行重大更改。

进行微调：对模型进行有监督的训练，使用训练集对模型进行微调，以最小化目标任务的损失函数。在微调过程中，新添加的顶层结构的参数将被更新，而预训练模型的其他参数保持不变。

评估和调整：使用验证集对微调后的模型进行评估，检查其性能，并进行必要的调整和优化。一旦达到满意的性能，使用测试集对最终模型进行评估，以评估其在新数据上的性能。

总的来说，微调预训练模型需要选择一个适当规模的、具有代表性的训练集，并仔细选择损失函数、优化器、学习率和其他超参数，以获得最佳的性能。